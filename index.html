<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Kaijing Ma </title> <meta name="author" content="Kaijing Ma"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="kaijing"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?53a094b51ed1d1e025731eb00d240058" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%98%8D&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kaijingofficial.github.io/"> <script src="/assets/js/theme.js?7b1068a1099d4262cace6933e385240d"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?46af317e693b09921dcb92261d123fbc" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Kaijing</span> Ma </h1> <p class="desc">Internship <a href="https://www.shlab.org.cn/" rel="external nofollow noopener" target="_blank">@Shanghai AI Lab <img src="assets/img/ailab.png" alt="logo of shailab" style="display:inline; width:30px; height:20px;"></a>, <br> Master student <a href="http://en.xjtu.edu.cn/" rel="external nofollow noopener" target="_blank">@XJTU <img src="assets/img/a4-2xbred.png" alt="logo of xjtu" style="display:inline; width:70px; height:20px;"></a> with <a href="https://www.teleagi.cn" rel="external nofollow noopener" target="_blank">@TeleAI <img src="assets/img/teleai.png" alt="logo of TeleAI" style="display:inline; width:20px; height:20px;"></a>, <br> Incoming PhD Student @FDU </p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/avatar.jpg" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/avatar.jpg?0b118e24b52a38a52af3e7b7a09c4c8c" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="avatar.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <a href="mailto:mail@kaijing.tech"><i class="fa-sharp fa-solid fa-at fa-2x"></i></a> <a href="https://scholar.google.com/citations?user=_6BqfrEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-google fa-2x"></i></a> <a href="https://github.com/KaijingOfficial" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github fa-2x"></i></a> </div> </div> <div class="clearfix"> <hr> <p><strong><em>â€œThe future is already here â€“ itâ€™s just not evenly distributed.â€</em></strong></p> <hr> <p>As a masterâ€™s student, I am enrolled in a joint training program at <strong><a href="https://www.xjtu.edu.cn" rel="external nofollow noopener" target="_blank">Xiâ€™an Jiaotong University</a></strong> and <strong><a href="https://www.teleagi.cn" rel="external nofollow noopener" target="_blank">TeleAI</a></strong>. My masterâ€™s studies are supervised collaboratively by Professor <strong>Xingsong Hou</strong> from XJTU and Professor <strong>Hao Sun</strong> from TeleAI.</p> <p>At TeleAI, I focus on building state-of-the-art multimodal understanding and generation models, such as Video Temporal Grounding and controllable AIGC systems. For example, we have released <strong><em>æ˜Ÿè¾°å¤šæ¨¡æ€å¤§æ¨¡å‹</em></strong> as a competitive text-to-image generation product for the public. Our team is led by IEEE Fellow <strong><a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Xuelong Li</a></strong>, who is also the CTO and Chief Scientist of <strong><a href="https://www.chinatelecomglobal.com/" rel="external nofollow noopener" target="_blank">China Telecom</a></strong>.</p> <p>Currently, I am interning at <a href="https://openrobotlab.org.cn/" rel="external nofollow noopener" target="_blank"><img src="assets/img/robot_log_dark.png" alt="logo of robotlab" style="display:inline; width:120px; height:25px; vertical-align: top;"></a>, under the supervision of Prof.<strong><a href="https://tonghe90.github.io/" rel="external nofollow noopener" target="_blank">Tong He</a></strong>. We are a young team attached to <strong><a href="https://www.shlab.org.cn/" rel="external nofollow noopener" target="_blank">Shanghai AI Laboratory</a></strong>, focusing on Embodied AI. I have a strong belief in developing Foundation Embodied AI Models with zero-shot cross-embodiment capabilities.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Sep 21, 2024</th> <td> Received the â€˜Excellence in Engineering Award (Student Category)â€™ from the <strong><a href="https://nse.xjtu.edu.cn/index.htm" rel="external nofollow noopener" target="_blank">National School for Engineers</a></strong> at Xiâ€™an Jiaotong UniversityğŸ¥³ </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 14, 2024</th> <td> We released the code of <strong><a href="https://github.com/KaijingOfficial/sram_vtg" rel="external nofollow noopener" target="_blank">SRAM</a></strong>ğŸ”¥ </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 15, 2024</th> <td> Our Paper About Controllable Text-to-image Generation is accepted to MM2024! </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 15, 2024</th> <td> Our paper about moment retrieval is accepted as oral paper in ICME 2024. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 15, 2023</th> <td> Our paper about moment retrieval is accepted to ICCV Workshop 2023. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICCVW</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/teaser_iccvw.png" sizes="200px"> <img src="/assets/img/publication_preview/teaser_iccvw.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="teaser_iccvw.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ma2023llavilo" class="col-sm-8"> <div class="title">LLaViLo: Boosting Video Moment Retrieval via Adapter-Based Multimodal Modeling</div> <div class="author"> <em>Kaijing Ma<sup>*</sup></em>,Â Xianghao Zang<sup>*</sup>,Â Zerun Feng,Â Han Fang,Â Chao Ban,Â Yuhan Wei,Â Zhongjiang He,Â Yongxiang Li,Â andÂ Hao Sun<sup>â€ </sup> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Ma_LLaViLo_Boosting_Video_Moment_Retrieval_via_Adapter-Based_Multimodal_Modeling_ICCVW_2023_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ma2023llavilo</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LLaViLo: Boosting Video Moment Retrieval via Adapter-Based Multimodal Modeling}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Kaijing and Zang, Xianghao and Feng, Zerun and Fang, Han and Ban, Chao and Wei, Yuhan and He, Zhongjiang and Li, Yongxiang and Sun, Hao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2798--2803}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/nips24.png" sizes="200px"> <img src="/assets/img/publication_preview/nips24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="nips24.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ma2024beyond" class="col-sm-8"> <div class="title">Beyond Uncertainty: Evidential Deep Learning for Robust Video Temporal Grounding</div> <div class="author"> <em>Kaijing Ma<sup>*</sup></em>,Â Haojian Huang<sup>*</sup>,Â Jin Chen<sup>*</sup>,Â Haodong Chen,Â Pengliang Ji,Â Xianghao Zang,Â Han Fang,Â Chao Ban,Â Hao Sun,Â Mulin Chen,Â andÂ  others </div> <div class="periodical"> <em>arXiv preprint arXiv:2408.16272</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2408.16272" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/KaijingOfficial/sram_vtg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ma2024beyond</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Beyond Uncertainty: Evidential Deep Learning for Robust Video Temporal Grounding}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Kaijing and Huang, Haojian and Chen, Jin and Chen, Haodong and Ji, Pengliang and Zang, Xianghao and Fang, Han and Ban, Chao and Sun, Hao and Chen, Mulin and others}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2408.16272}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACM MM</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mm24.png" sizes="200px"> <img src="/assets/img/publication_preview/mm24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mm24.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="li2024goal" class="col-sm-8"> <div class="title">GOAL: Grounded text-to-image Synthesis with Joint Layout Alignment Tuning</div> <div class="author"> Yaqi Li,Â Han Fang,Â Zerun Feng,Â <em>Kaijing Ma</em>,Â Chao Ban,Â Xianghao Zang,Â LanXiang Zhou,Â Zhongjiang He,Â Jingyan Chen,Â Jiani Hu,Â andÂ  others </div> <div class="periodical"> <em>In ACM Multimedia 2024</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/forum?id=N6hFKeLOfu" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2024goal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GOAL: Grounded text-to-image Synthesis with Joint Layout Alignment Tuning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Yaqi and Fang, Han and Feng, Zerun and Ma, Kaijing and Ban, Chao and Zang, Xianghao and Zhou, LanXiang and He, Zhongjiang and Chen, Jingyan and Hu, Jiani and others}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM Multimedia 2024}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tuned.jpg" sizes="200px"> <img src="/assets/img/publication_preview/tuned.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tuned.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="huang2024trusted" class="col-sm-8"> <div class="title">Trusted Unified Feature-Neighborhood Dynamics for Multi-View Classification</div> <div class="author"> Haojian Huang,Â Chuanyu Qin,Â Zhe Liu,Â <em>Kaijing Ma</em>,Â Jin Chen,Â Han Fang,Â Chao Ban,Â Hao Sun,Â andÂ Zhongjiang He<sup>â€ </sup> </div> <div class="periodical"> <em>arXiv preprint arXiv:2409.00755</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2409.00755" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/JethroJames/TUNED" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">huang2024trusted</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Trusted Unified Feature-Neighborhood Dynamics for Multi-View Classification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Haojian and Qin, Chuanyu and Liu, Zhe and Ma, Kaijing and Chen, Jin and Fang, Han and Ban, Chao and Sun, Hao and He, Zhongjiang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2409.00755}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/bovila.png" sizes="200px"> <img src="/assets/img/publication_preview/bovila.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bovila.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="chen2024bovila" class="col-sm-8"> <div class="title">BoViLA: Bootstrapping Video-Language Alignment via LLM-Based Self-Questioning and Answering</div> <div class="author"> Jin Chen,Â <em>Kaijing Ma</em>,Â Haojian Huang,Â Jiayu Shen,Â Han Fang,Â Xianghao Zang,Â Chao Ban,Â Zhongjiang He,Â Hao Sun,Â andÂ Yanmei Kang </div> <div class="periodical"> <em>arXiv preprint arXiv:2410.02768</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2410.02768" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/dunknsabsw/BoViLA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chen2024bovila</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BoViLA: Bootstrapping Video-Language Alignment via LLM-Based Self-Questioning and Answering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Jin and Ma, Kaijing and Huang, Haojian and Shen, Jiayu and Fang, Han and Zang, Xianghao and Ban, Chao and He, Zhongjiang and Sun, Hao and Kang, Yanmei}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2410.02768}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2024 Kaijing Ma. Last updated: December 02, 2024. <img src="https://badges.toozhao.com/badges/01J8CPFQCV39TJDF56E3RY8TCG/blue.svg"> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?b977fe0c21b2118ed853308b1b923969"></script> <script src="/assets/js/no_defer.js?699fa7cbe3b29f831db7d5250ba3203a"></script> <script defer src="/assets/js/common.js?d3a25b46bbd2e0a751a27b173abc6e5f"></script> <script defer src="/assets/js/copy_code.js?d359581efc54b08366f9ef8219e6e511" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?25eff8ff4144a010e4ad7b31403102cf"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>